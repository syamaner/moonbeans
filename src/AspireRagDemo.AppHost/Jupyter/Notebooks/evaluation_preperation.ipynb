{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb96146-c613-4f66-bdbf-61069d45e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: libmagic in /opt/conda/lib/python3.12/site-packages (1.0)\n",
      "Cloning into 'docs-aspire'...\n",
      "remote: Enumerating objects: 15189, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
      "remote: Total 15189 (delta 14), reused 10 (delta 4), pack-reused 15157 (from 2)\u001b[K\n",
      "Receiving objects: 100% (15189/15189), 60.00 MiB | 1.80 MiB/s, done.\n",
      "Resolving deltas: 100% (9923/9923), done.\n"
     ]
    }
   ],
   "source": [
    "# This notebook is  used to evaluate perofrmance of our RAG ingestion and query pipeline.\n",
    "# Method:\n",
    "#   - Generated test data using a sample GitHub repository (offline process not in this notebook)\n",
    "#     - https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/\n",
    "#   - Use our ingestion pipeline to parse the same reporisiroty and index in our vector store\n",
    "#   - Use RAGAS on teadt dataset + our answers fromn our RAG and visualise the mterics.\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(1, '/home/jovyan/work/code')\n",
    "from opentelemetry import trace\n",
    "from config import VectorDBConfig, EmbeddingConfig, ProcessingConfig, ChatConfig\n",
    "from config_helper import ConfigHelper\n",
    "from pipeline import DocumentPipeline\n",
    "from localrag import LocalRAG\n",
    "import nltk\n",
    "\n",
    "# We are using Aspire. Of course we will see the telemetry and logs in our dashboard!\n",
    "# see config_helper.py for the not to tidy details.\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "config_helper = ConfigHelper(True)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "!git clone https://github.com/dotnet/docs-aspire\n",
    "directory = \"docs-aspire\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5fd99-c673-423f-bdf3-674ca2feab41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c10dad532a4e0aa9e19e51f9542da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f930e2bc2bb94b88b2e72fb35e38a75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1d7e576b7148cf9d717e524710ef93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "path = \"docs-aspire/\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
    "\n",
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3261a3-c3fb-4173-a44f-b9bb1e82d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for doc in docs:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# define your LLM and Embedding Model\n",
    "# here we are using the same LLM and Embedding Model that we used to generate the testset\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = generator_embeddings\n",
    "\n",
    "trans = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
    "apply_transforms(kg, trans)\n",
    "kg.save(\"knowledge_graph.json\")\n",
    "loaded_kg = KnowledgeGraph.load(\"knowledge_graph.json\")\n",
    "loaded_kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162e495-35b3-41b2-a3e7-562c6d16c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=loaded_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad923f-aa04-46aa-8052-f90f613e46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "\n",
    "query_distribution = default_query_distribution(generator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0d1a9-face-4248-b560-3cf4bb5fa2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = generator.generate(testset_size=15, query_distribution=query_distribution)\n",
    "testset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e420af-10e9-41c0-8ab1-660add16a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "type(testset)\n",
    "file_name= \"test_data__aspire_15.pkl\"\n",
    "df = testset.to_pandas()\n",
    "df.to_pickle(file_name) \n",
    "df1 =  pd.read_pickle(file_name)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757e336-7b98-44af-ae16-716b15d446bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "def demonstrate_local_rag(rag):\n",
    "    \"\"\"Demonstrate how to use the LocalRAG class.\"\"\"    \n",
    "    # Example questions to test\n",
    "    questions = [\n",
    "        \"Why should I know about .Net Aspire?\",\n",
    "        \"Is .Net Aspire an alternative to Kubernetes?\"\n",
    "    ]\n",
    "    with tracer.start_as_current_span(\"Entering questions loop.\"):\n",
    "        for question in questions:\n",
    "            print(f\"Question: {question}\")\n",
    "            print(\"\\nRelevant chunks:\")\n",
    "            #with tracer.start_as_current_span(\"rag get chunks\"):\n",
    "            # chunks = rag.get_relevant_chunks(question, k=5)\n",
    "            # for i, chunk in enumerate(chunks, 1):\n",
    "            #     print(f\"\\nChunk {i}:\")\n",
    "            #     print(f\"Source: {chunk.metadata.get('file_path', 'Unknown')}\")\n",
    "            #      print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "                \n",
    "            print(\"\\nGenerated Answer:\")\n",
    "            with tracer.start_as_current_span(\"Retrieve answers.\"):\n",
    "                answer = rag.retrieve_and_answer(question, k=6)\n",
    "                print(answer)\n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "rag = LocalRAG(\n",
    "    vector_db_config=vector_db_config,\n",
    "    embedding_config=embedding_config, \n",
    "    chat_config=chat_config\n",
    ")\n",
    "with tracer.start_as_current_span(\"Starting demo\"):\n",
    "    demonstrate_local_rag(rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a3306-7e39-404d-be32-c4b947167703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
