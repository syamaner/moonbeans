{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb96146-c613-4f66-bdbf-61069d45e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is  used to evaluate perofrmance of our RAG ingestion and query pipeline.\n",
    "# Method:\n",
    "#   - Generated test data using a sample GitHub repository (offline process not in this notebook)\n",
    "#     - https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/\n",
    "#   - Use our ingestion pipeline to parse the same reporisiroty and index in our vector store\n",
    "#   - Use RAGAS on teadt dataset + our answers fromn our RAG and visualise the mterics.\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(1, '/home/jovyan/work/code')\n",
    "from opentelemetry import trace\n",
    "from config import VectorDBConfig, EmbeddingConfig, ProcessingConfig, ChatConfig\n",
    "from config_helper import ConfigHelper\n",
    "from pipeline import DocumentPipeline\n",
    "from localrag import LocalRAG\n",
    "\n",
    "# We are using Aspire. Of course we will see the telemetry and logs in our dashboard!\n",
    "# see config_helper.py for the not to tidy details.\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "config_helper = ConfigHelper(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e420af-10e9-41c0-8ab1-660add16a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the test data\n",
    "# The test data is generated using the method described at: \n",
    "#    https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/\n",
    "file_name= \"test_data__aspire_15.pkl\" \n",
    "test_dataset =  pd.read_pickle(file_name)\n",
    "test_dataset.head()\n",
    "#for index, row in test_dataset.iterrows():\n",
    "#    print(row[\"reference_contexts\"])\n",
    "#    reference= row[\"reference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c1180-73e9-40b4-a49b-d6a03704b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to ingest the test data into our vector store so that we can query later for evaluation\n",
    "\n",
    "pipeline = DocumentPipeline(\n",
    "    vector_db_config=config_helper.vector_db_config,\n",
    "    embedding_config=config_helper.embedding_config\n",
    ")\n",
    "\n",
    "repository=\"https://github.com/dotnet/docs-aspire\"\n",
    "input_file_name=\"merged_output.txt\"\n",
    "with tracer.start_as_current_span(f\"Starting ingesting file {input_file_name}\"):\n",
    "    pipeline.process_single_file(input_file_name,repository)\n",
    "\n",
    "# https://learning.oreilly.com/library/view/learning-langchain/9781098167271/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757e336-7b98-44af-ae16-716b15d446bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.ragas.io/en/latest/getstarted/rag_eval/#basic-setup\n",
    "evaluation_data=[]\n",
    "\n",
    "def query_using_rag(rag, question): \n",
    "    references=[]\n",
    "    with tracer.start_as_current_span(\"Getting answer and context.\"): \n",
    "        print(f\"Question: {question}\")\n",
    "        with tracer.start_as_current_span(\"rag get context\"):\n",
    "            chunks = rag.get_relevant_chunks(question, k=5)\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                references.append(chunk.page_content)\n",
    "        with tracer.start_as_current_span(\"Retrieve answers.\"):\n",
    "            answer = rag.retrieve_and_answer(question, k=5)\n",
    "            return (answer, references)\n",
    "\n",
    "rag = LocalRAG(\n",
    "    vector_db_config=config_helper.vector_db_config,\n",
    "    embedding_config=config_helper.embedding_config, \n",
    "    chat_config=config_helper.chat_config\n",
    ")\n",
    "\n",
    "with tracer.start_as_current_span(\"Starting demo\"):\n",
    "    for index, row in test_dataset.iterrows():\n",
    "        print(f\"Question {index}:\")\n",
    "        question = row[\"user_input\"]\n",
    "        reference= row[\"reference\"]\n",
    "        answer,contexts = query_using_rag(rag, question)        \n",
    "        print(answer[:50])\n",
    "        evaluation_data.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"response\": answer,\n",
    "            \"reference\": reference\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d7aa8-3928-4f58-abb5-730a65d647c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Save the eval data so we can skip this step for the same dataset next time.\n",
    "if not os.path.exists('eval_data_with_answers_50.pkl'):\n",
    "    with open('eval_data_with_answers_50.pkl', 'wb') as f:\n",
    "        pickle.dump(evaluation_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a3306-7e39-404d-be32-c4b947167703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas import EvaluationDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, \n",
    "    Faithfulness, \n",
    "    FactualCorrectness, \n",
    "    AnswerRelevancy,\n",
    "    ContextRelevancy\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(), \n",
    "    Faithfulness(), \n",
    "    FactualCorrectness(),\n",
    "    AnswerRelevancy(),\n",
    "    ContextRelevancy()\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "evaluation_dataset = EvaluationDataset.from_list(evaluation_data)\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "with tracer.start_as_current_span(\"Starting model evaluation\"):\n",
    "    result = evaluate(dataset=evaluation_dataset,metrics=metrics,llm=evaluator_llm)\n",
    "    print(result)\n",
    "\n",
    "    \n",
    "# Visualization\n",
    "def visualize_rag_metrics(result):\n",
    "    # Extract metric names and scores\n",
    "    metric_names = [metric.__class__.__name__ for metric in metrics]\n",
    "    scores = [result[metric_name] for metric_name in metric_names]\n",
    "\n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(metric_names, scores, color='skyblue', edgecolor='navy')\n",
    "    plt.title('RAG Evaluation Metrics', fontsize=15)\n",
    "    plt.xlabel('Metrics', fontsize=12)\n",
    "    plt.ylabel('Scores', fontsize=12)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Add a horizontal line at 0.5 for reference\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "visualize_rag_metrics(result)\n",
    "    \n",
    "#{'context_recall': 0.5450, 'faithfulness': 0.5920, 'factual_correctness': 0.3941}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c648bdd-4c67-41e5-8dba-de3579ebd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from ragas import evaluate\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.metrics import (\n",
    "#     LLMContextRecall, \n",
    "#     Faithfulness, \n",
    "#     FactualCorrectness, \n",
    "#     AnswerRelevancy,\n",
    "#     ContextRelevancy\n",
    "# )\n",
    "# # from ragas import EvaluationDataset\n",
    "\n",
    "# # # Initialize components\n",
    "# # llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# # embeddings = OpenAIEmbeddings()\n",
    "# # evaluation_dataset = EvaluationDataset.from_list(evaluation_data)\n",
    "# # evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# # Define metrics\n",
    "# metrics = [\n",
    "#     LLMContextRecall(), \n",
    "#     Faithfulness(), \n",
    "#     FactualCorrectness(),\n",
    "#     AnswerRelevancy(),\n",
    "#     ContextRelevancy()\n",
    "# ]\n",
    "\n",
    "# # Evaluate\n",
    "# with tracer.start_as_current_span(\"Starting model evaluation\"):\n",
    "#     result = evaluate(\n",
    "#         dataset=evaluation_dataset,\n",
    "#         metrics=metrics,\n",
    "#         llm=evaluator_llm\n",
    "#     )\n",
    "#     print(result)\n",
    "\n",
    "# # Visualization\n",
    "# def visualize_rag_metrics(result):\n",
    "#     # Extract metric names and scores\n",
    "#     metric_names = [metric.__class__.__name__ for metric in metrics]\n",
    "#     scores = [result[metric_name] for metric_name in metric_names]\n",
    "\n",
    "#     # Create bar plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     bars = plt.bar(metric_names, scores, color='skyblue', edgecolor='navy')\n",
    "#     plt.title('RAG Evaluation Metrics', fontsize=15)\n",
    "#     plt.xlabel('Metrics', fontsize=12)\n",
    "#     plt.ylabel('Scores', fontsize=12)\n",
    "#     plt.ylim(0, 1)\n",
    "\n",
    "#     # Add value labels on top of each bar\n",
    "#     for bar in bars:\n",
    "#         height = bar.get_height()\n",
    "#         plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "#                  f'{height:.2f}',\n",
    "#                  ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "#     # Add a horizontal line at 0.5 for reference\n",
    "#     plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_rag_metrics(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
