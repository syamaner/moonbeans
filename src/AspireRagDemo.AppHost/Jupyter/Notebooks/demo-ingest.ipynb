{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8318965e-48b2-4977-9580-25cf3da49bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Qdrant connection string: Endpoint=http://qdrant:6333;Key=aMjJKx0t1a6E9hysaCacWz\n",
      "Successfully connected to Qdrant!\n",
      "Available collections: collections=[CollectionDescription(name='embedding-demo'), CollectionDescription(name='embedding-demo1'), CollectionDescription(name='my_repo_collection')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13742/3189862644.py:35: UserWarning: Api key is used with an insecure connection.\n",
      "  qdrant = QdrantClient(url=endpoint, api_key=api_key)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "from gitingest import ingest\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    Language,\n",
    "    PythonCodeTextSplitter\n",
    ")\n",
    " \n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from qdrant_client import QdrantClient\n",
    "import yaml\n",
    "import json\n",
    "from qdrant_client.http import models as rest\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get connection string from environment variable or use default\n",
    "qdrant_conn = os.getenv('ConnectionStrings__qdrant_http')\n",
    "print(f'Using Qdrant connection string: {qdrant_conn}')\n",
    "\n",
    "# Parse connection string\n",
    "endpoint = qdrant_conn.split(';')[0].split('=')[1]\n",
    "api_key = qdrant_conn.split(';')[1].split('=')[1]\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant = QdrantClient(url=endpoint, api_key=api_key)\n",
    "\n",
    "# Setup Ollama Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\", \n",
    "    base_url=\"http://ollama:11434\"\n",
    ")\n",
    "\n",
    "# Test connection by listing collections\n",
    "collections = qdrant.get_collections()\n",
    "print(\"Successfully connected to Qdrant!\")\n",
    "print(f\"Available collections: {collections}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CodeEntity:\n",
    "    \"\"\"Represents a code entity (function, class, etc.) with its metadata.\"\"\"\n",
    "    name: str\n",
    "    type: str  # 'function', 'class', 'method'\n",
    "    docstring: Optional[str]\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    decorators: List[str]\n",
    "    parent: Optional[str]\n",
    "    dependencies: List[str]\n",
    "\n",
    "class MetadataExtractor:\n",
    "    \"\"\"Extracts rich metadata from different file types.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_python_entities(content: str) -> List[CodeEntity]:\n",
    "        \"\"\"Extract functions, classes, and methods from Python code.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            entities = []\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                    # Extract docstring\n",
    "                    docstring = ast.get_docstring(node)\n",
    "                    \n",
    "                    # Get decorators\n",
    "                    decorators = [\n",
    "                        ast.unparse(decorator).strip()\n",
    "                        for decorator in node.decorator_list\n",
    "                    ]\n",
    "                    \n",
    "                    # Find imports and dependencies\n",
    "                    dependencies = []\n",
    "                    for sub_node in ast.walk(node):\n",
    "                        if isinstance(sub_node, ast.Import):\n",
    "                            dependencies.extend(n.name for n in sub_node.names)\n",
    "                        elif isinstance(sub_node, ast.ImportFrom):\n",
    "                            dependencies.append(sub_node.module)\n",
    "                    \n",
    "                    entity = CodeEntity(\n",
    "                        name=node.name,\n",
    "                        type='class' if isinstance(node, ast.ClassDef) else 'function',\n",
    "                        docstring=docstring,\n",
    "                        start_line=node.lineno,\n",
    "                        end_line=node.end_lineno,\n",
    "                        decorators=decorators,\n",
    "                        parent=None,  # Will be filled later for methods\n",
    "                        dependencies=list(set(dependencies))\n",
    "                    )\n",
    "                    entities.append(entity)\n",
    "            \n",
    "            return entities\n",
    "        except SyntaxError:\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_markdown_metadata(content: str) -> Dict:\n",
    "        \"\"\"Extract metadata from markdown files.\"\"\"\n",
    "        metadata = {\n",
    "            'headers': [],\n",
    "            'links': [],\n",
    "            'code_blocks': [],\n",
    "            'frontmatter': None\n",
    "        }\n",
    "        \n",
    "        # Extract headers\n",
    "        headers = re.findall(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE)\n",
    "        metadata['headers'] = [(len(h[0]), h[1]) for h in headers]\n",
    "        \n",
    "        # Extract links\n",
    "        links = re.findall(r'\\[([^\\]]+)\\]\\(([^\\)]+)\\)', content)\n",
    "        metadata['links'] = links\n",
    "        \n",
    "        # Extract code blocks\n",
    "        code_blocks = re.findall(r'```(\\w+)?\\n(.*?)```', content, re.DOTALL)\n",
    "        metadata['code_blocks'] = [(lang or 'text', code) for lang, code in code_blocks]\n",
    "        \n",
    "        # Extract frontmatter\n",
    "        if content.startswith('---'):\n",
    "            try:\n",
    "                fm_match = re.match(r'---\\n(.*?)\\n---', content, re.DOTALL)\n",
    "                if fm_match:\n",
    "                    metadata['frontmatter'] = yaml.safe_load(fm_match.group(1))\n",
    "            except yaml.YAMLError:\n",
    "                pass\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_html_template_metadata(content: str, file_type: str) -> Dict:\n",
    "        \"\"\"Extract metadata from HTML/Jinja templates.\"\"\"\n",
    "        metadata = {\n",
    "            'blocks': [],\n",
    "            'extends': None,\n",
    "            'includes': [],\n",
    "            'macros': [],\n",
    "            'variables': []\n",
    "        }\n",
    "        \n",
    "        if file_type == '.jinja':\n",
    "            # Extract template inheritance\n",
    "            extends_match = re.search(r'{%\\s*extends\\s+[\\'\"](.+?)[\\'\"]', content)\n",
    "            if extends_match:\n",
    "                metadata['extends'] = extends_match.group(1)\n",
    "            \n",
    "            # Extract blocks\n",
    "            blocks = re.findall(r'{%\\s*block\\s+(\\w+)\\s*%}', content)\n",
    "            metadata['blocks'] = blocks\n",
    "            \n",
    "            # Extract includes\n",
    "            includes = re.findall(r'{%\\s*include\\s+[\\'\"](.+?)[\\'\"]', content)\n",
    "            metadata['includes'] = includes\n",
    "            \n",
    "            # Extract macros\n",
    "            macros = re.findall(r'{%\\s*macro\\s+(\\w+)\\s*\\(', content)\n",
    "            metadata['macros'] = macros\n",
    "            \n",
    "            # Extract variables\n",
    "            variables = re.findall(r'{{(.+?)}}', content)\n",
    "            metadata['variables'] = [v.strip() for v in variables]\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "def create_chunking_strategies():\n",
    "    \"\"\"Create specialized chunking strategies for different file types.\"\"\"\n",
    "    return {\n",
    "        # Python files\n",
    "        '.py': RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.PYTHON,\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            #separators=[\"\\nclass \", \"\\ndef \", \"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        \n",
    "        # Markdown files\n",
    "        '.md': MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "                (\"####\", \"Header 4\")\n",
    "            ]\n",
    "        ),\n",
    "        \n",
    "        # HTML/Jinja templates\n",
    "        '.html': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"</div>\", \"</template>\", \"</section>\", \"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        '.jinja': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"{% block \", \"{% extends \", \"{% include \", \"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        \n",
    "        # Config files\n",
    "        '.yml': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"---\", \"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        '.yaml': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"---\", \"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        '.toml': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        \n",
    "        # Other common file types\n",
    "        '.json': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"},\", \"}\\n\", \"\\n\"]\n",
    "        ),\n",
    "        '.rst': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=600,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"\\n=+\\n\", \"\\n-+\\n\", \"\\n\\n\", \"\\n\"]\n",
    "        ),\n",
    "        '.txt': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    "        ),\n",
    "        \n",
    "        # Default\n",
    "        'default': RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def extract_file_metadata(file_path: str, content: str) -> Dict:\n",
    "    \"\"\"Extract comprehensive metadata for a file.\"\"\"\n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    metadata = {\n",
    "        \"file_path\": file_path,\n",
    "        \"file_type\": file_ext,\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"directory\": os.path.dirname(file_path),\n",
    "        \"size_bytes\": len(content.encode('utf-8')),\n",
    "        \"num_lines\": len(content.splitlines()),\n",
    "        \"is_empty\": len(content.strip()) == 0,\n",
    "        \"has_shebang\": content.startswith('#!') if content else False,\n",
    "        \"file_level_metadata\": {}\n",
    "    }\n",
    "    \n",
    "    # Extract file type specific metadata\n",
    "    if file_ext == '.py':\n",
    "        python_entities = MetadataExtractor.extract_python_entities(content)\n",
    "        metadata['file_level_metadata'].update({\n",
    "            'classes': [e for e in python_entities if e.type == 'class'],\n",
    "            'functions': [e for e in python_entities if e.type == 'function'],\n",
    "            'has_main': any(e.name == '__main__' for e in python_entities),\n",
    "            'imports': re.findall(r'^(?:from|import)\\s+(\\S+)', content, re.MULTILINE),\n",
    "            'doc_coverage': sum(1 for e in python_entities if e.docstring) / len(python_entities) if python_entities else 0\n",
    "        })\n",
    "    \n",
    "    elif file_ext == '.md':\n",
    "        metadata['file_level_metadata'].update(\n",
    "            MetadataExtractor.extract_markdown_metadata(content)\n",
    "        )\n",
    "    \n",
    "    elif file_ext in ['.html', '.jinja']:\n",
    "        metadata['file_level_metadata'].update(\n",
    "            MetadataExtractor.extract_html_template_metadata(content, file_ext)\n",
    "        )\n",
    "    \n",
    "    elif file_ext in ['.yml', '.yaml']:\n",
    "        try:\n",
    "            yaml_content = yaml.safe_load(content)\n",
    "            metadata['file_level_metadata']['yaml_structure'] = {\n",
    "                'top_level_keys': list(yaml_content.keys()) if isinstance(yaml_content, dict) else [],\n",
    "                'is_list': isinstance(yaml_content, list)\n",
    "            }\n",
    "        except yaml.YAMLError:\n",
    "            pass\n",
    "    \n",
    "    elif file_ext == '.json':\n",
    "        try:\n",
    "            json_content = json.loads(content)\n",
    "            metadata['file_level_metadata']['json_structure'] = {\n",
    "                'top_level_keys': list(json_content.keys()) if isinstance(json_content, dict) else [],\n",
    "                'is_array': isinstance(json_content, list)\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return metadata\n",
    "    \n",
    "\n",
    "# 2. Let's modify split_by_files to be more verbose and handle errors better\n",
    "def split_by_files(content: str) -> List[Dict]:\n",
    "    \"\"\"Split the concatenated content into individual files with their paths.\"\"\"\n",
    "    if not content:\n",
    "        print(\"Warning: Content is empty\")\n",
    "        return []\n",
    "    \n",
    "    # Split by double newline to handle the actual format\n",
    "    file_parts = content.split(\"\\n\\n\")\n",
    "    files = []\n",
    "    \n",
    "    current_file = None\n",
    "    current_content = []\n",
    "    \n",
    "    for part in file_parts:\n",
    "        if part.startswith(\"================================================\\nFile: \"):\n",
    "            # If we have a previous file, save it\n",
    "            if current_file:\n",
    "                files.append({\n",
    "                    \"path\": current_file,\n",
    "                    \"content\": \"\\n\".join(current_content),\n",
    "                    \"metadata\": extract_file_metadata(current_file, \"\\n\".join(current_content))\n",
    "                })\n",
    "                current_content = []\n",
    "            \n",
    "            # Extract new file path\n",
    "            file_line = part.split('\\n')[1]  # Get the \"File: /path\" line\n",
    "            current_file = file_line.replace(\"File: \", \"\").strip()\n",
    "        else:\n",
    "            if current_file and part.strip():  # Only add non-empty parts\n",
    "                current_content.append(part)\n",
    "    \n",
    "    # Don't forget to add the last file\n",
    "    if current_file:\n",
    "        files.append({\n",
    "            \"path\": current_file,\n",
    "            \"content\": \"\\n\".join(current_content),\n",
    "            \"metadata\": extract_file_metadata(current_file, \"\\n\".join(current_content))\n",
    "        })\n",
    "    \n",
    "    print(f\"Total files processed: {len(files)}\")\n",
    "    if files:\n",
    "        print(f\"Sample file paths:\")\n",
    "        for i, file in enumerate(files[:3]):  # Show first 3 files\n",
    "            print(f\"  {i+1}. {file['path']}\")\n",
    "    \n",
    "    return files\n",
    " \n",
    "\n",
    "# 3. Let's modify advanced_repository_chunking to be more verbose\n",
    "def advanced_repository_chunking(content: str, repo_url: str):\n",
    "    \"\"\"Process repository content using file-type specific chunking with rich metadata.\"\"\"\n",
    "    print(\"Starting repository chunking...\")\n",
    "    \n",
    "    # Get chunking strategies\n",
    "    chunking_strategies = create_chunking_strategies()\n",
    "    print(\"Created chunking strategies\")\n",
    "    \n",
    "    # Split content into files\n",
    "    files = split_by_files(content)\n",
    "    print(f\"Split content into {len(files)} files\")\n",
    "    \n",
    "    if not files:\n",
    "        print(\"Warning: No files to process\")\n",
    "        return []\n",
    "    \n",
    "    # Extract repository-level metadata\n",
    "    repo_metadata = {\n",
    "        \"repository_url\": repo_url,\n",
    "        \"repository_name\": repo_url.split('/')[-1],\n",
    "        \"organization\": repo_url.split('/')[-2],\n",
    "        \"total_files\": len(files),\n",
    "        \"file_types\": {},\n",
    "        \"directory_structure\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each file with appropriate chunking strategy\n",
    "    processed_chunks = []\n",
    "    for file in files:\n",
    "        file_ext = file[\"metadata\"][\"file_type\"]\n",
    "        print(f\"Processing file with extension: {file_ext}\")\n",
    "        \n",
    "        splitter = chunking_strategies.get(file_ext, chunking_strategies['default'])\n",
    "        combined_metadata = {\n",
    "            **file[\"metadata\"],\n",
    "            \"repository\": repo_metadata\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if file_ext == '.md':\n",
    "                header_splits = splitter.split_text(file[\"content\"])\n",
    "                if any(len(split.page_content) > 600 for split in header_splits):\n",
    "                    size_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=600,\n",
    "                        chunk_overlap=50,\n",
    "                        separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    "                    )\n",
    "                    for split in header_splits:\n",
    "                        smaller_splits = size_splitter.create_documents(\n",
    "                            texts=[split.page_content],\n",
    "                            metadatas=[{\n",
    "                                **split.metadata,\n",
    "                                **combined_metadata\n",
    "                            }]\n",
    "                        )\n",
    "                        processed_chunks.extend(smaller_splits)\n",
    "                else:\n",
    "                    for split in header_splits:\n",
    "                        split.metadata.update(combined_metadata)\n",
    "                    processed_chunks.extend(header_splits)\n",
    "            else:\n",
    "                chunks = splitter.create_documents(\n",
    "                    texts=[file[\"content\"]],\n",
    "                    metadatas=[combined_metadata]\n",
    "                )\n",
    "                processed_chunks.extend(chunks)\n",
    "                print(f\"Added {len(chunks)} chunks for file {file['path']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file['path']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Total chunks created: {len(processed_chunks)}\")\n",
    "    if processed_chunks:\n",
    "        print(\"Sample chunk metadata:\", processed_chunks[0].metadata)\n",
    "    return processed_chunks\n",
    "\n",
    "\n",
    "def setup_qdrant(collection_name: str = \"code_repositories\"):\n",
    "    \"\"\"Setup Qdrant connection and initialize embeddings.\"\"\"\n",
    "    # Get connection string from environment variable or use default\n",
    "    qdrant_conn = os.getenv('ConnectionStrings__qdrant_http')\n",
    "    if not qdrant_conn:\n",
    "        raise ValueError(\"Qdrant connection string not found in environment variables\")\n",
    "    \n",
    "    # Parse connection string\n",
    "    endpoint = qdrant_conn.split(';')[0].split('=')[1]\n",
    "    api_key = qdrant_conn.split(';')[1].split('=')[1]\n",
    "    \n",
    "    # Initialize Qdrant client\n",
    "    qdrant = QdrantClient(url=endpoint, api_key=api_key)\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings\n",
    "# 5. Create vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant,\n",
    "    collection_name=\"my_repo_collection\",\n",
    "    embedding=embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ca6b7a-56bd-4b95-89d2-26224a2c1964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: cyclotruc/gitingest\n",
      "Files analyzed: 51\n",
      "\n",
      "Estimated tokens: 51.2k\n",
      "Starting repository chunking...\n",
      "Created chunking strategies\n",
      "Total files processed: 1\n",
      "Sample file paths:\n",
      "  1. /README.md\n",
      "Split content into 1 files\n",
      "Processing file with extension: .md\n",
      "Total chunks created: 420\n",
      "Sample chunk metadata: {'file_path': '/README.md', 'file_type': '.md', 'file_name': 'README.md', 'directory': '/', 'size_bytes': 209951, 'num_lines': 5024, 'is_empty': False, 'has_shebang': False, 'file_level_metadata': {'headers': [(2, 'üöÄ Features'), (2, 'üì¶ Installation'), (2, 'üß© Browser Extension Usage'), (2, 'üí° Command line usage'), (1, 'Basic usage'), (1, 'From URL'), (1, 'See more options'), (2, 'üêõ Python package usage'), (1, 'or from URL'), (2, 'üåê Self-host'), (2, '‚úîÔ∏è Contributing to Gitingest'), (3, 'Non-technical ways to contribute'), (3, 'Technical ways to contribute'), (2, 'üõ†Ô∏è Stack'), (3, 'Looking for a JavaScript/Node package?'), (2, 'Project Growth'), (1, 'Contributing to Gitingest'), (2, 'How to Contribute (non-technical)'), (2, 'How to submit a Pull Request'), (1, 'Build stage'), (1, 'Copy requirements first to leverage Docker cache'), (1, 'Install build dependencies and Python packages'), (1, 'Runtime stage'), (1, 'Set Python environment variables'), (1, 'Install Git'), (1, 'Create a non-root user'), (1, 'Change ownership of the application files'), (1, 'Switch to non-root user'), (1, 'Git'), (1, 'Python'), (1, 'Virtual environment'), (1, 'IDE'), (1, 'Project specific'), (1, 'Contributor Covenant Code of Conduct'), (2, 'Our Pledge'), (2, 'Our Standards'), (2, 'Enforcement Responsibilities'), (2, 'Scope'), (2, 'Enforcement'), (2, 'Enforcement Guidelines'), (3, '1. Correction'), (3, '2. Warning'), (3, '3. Temporary Ban'), (3, '4. Permanent Ban'), (2, 'Attribution'), (1, 'Security Policy'), (2, 'Reporting a Vulnerability'), (1, 'Initialize a rate limiter'), (2, 'Color printing utility'), (1, 'Load environment variables from .env file'), (1, 'Initialize the FastAPI application with lifespan'), (1, 'Register the custom exception handler for rate limits'), (1, 'Mount static files to serve CSS, JS, and other static assets'), (1, 'Set up API analytics middleware if an API key is provided'), (1, 'Fetch allowed hosts from the environment or use the default values'), (1, 'Add middleware to enforce allowed hosts'), (1, 'Set up template rendering'), (1, 'Include routers for modular endpoints'), (1, 'pylint: disable=no-value-for-parameter'), (1, 'Linting configuration'), (1, 'Test configuration'), (1, \"Test that when using a ['*.txt'] as include pattern, only .txt files are processed & .py files are excluded\"), (1, 'single folder patterns'), (1, 'multiple patterns'), (1, \"TODO: test with multiple include patterns: ['*.txt', '*.py']\"), (1, \"TODO: test with multiple include patterns: ['/src/*', '*.txt']\"), (1, \"TODO: test with multiple include patterns: ['/src*', '*.txt']\")], 'links': [('![Image', './docs/frontpage.png \"Gitingest main page\"'), ('![License', 'https://img.shields.io/badge/license-MIT-blue.svg'), ('![PyPI version', 'https://badge.fury.io/py/gitingest.svg'), ('![GitHub stars', 'https://img.shields.io/github/stars/cyclotruc/gitingest?style=social.svg'), ('![Downloads', 'https://pepy.tech/badge/gitingest'), ('![Discord', 'https://dcbadge.limes.pink/api/server/https://discord.com/invite/zerRaGK9EC'), ('gitingest.com', 'https://gitingest.com'), ('Chrome Extension', 'https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood'), ('Firefox Add-on', 'https://addons.mozilla.org/firefox/addon/gitingest'), ('lcandy2/gitingest-extension', 'https://github.com/lcandy2/gitingest-extension'), ('create an issue', 'https://github.com/cyclotruc/gitingest/issues/new'), ('creating an issue', 'https://github.com/cyclotruc/gitingest/issues/new'), ('Discord', 'https://discord.com/invite/zerRaGK9EC'), ('Discord', 'https://discord.com/invite/zerRaGK9EC'), ('CONTRIBUTING.md', './CONTRIBUTING.md'), ('Tailwind CSS', 'https://tailwindcss.com'), ('FastAPI', 'https://github.com/fastapi/fastapi'), ('Jinja2', 'https://jinja.palletsprojects.com'), ('tiktoken', 'https://github.com/openai/tiktoken'), ('apianalytics.dev', 'https://www.apianalytics.dev'), ('![Star History Chart', 'https://api.star-history.com/svg?repos=cyclotruc/gitingest&type=Date'), ('Discord', 'https://discord.com/invite/zerRaGK9EC'), ('create an issue', 'https://github.com/cyclotruc/gitingest/issues/new'), ('creating an issue', 'https://github.com/cyclotruc/gitingest/issues/new'), ('Discord', 'https://discord.com/invite/zerRaGK9EC'), ('Contributor Covenant', 'https://www.contributor-covenant.org'), (\"Mozilla's code of conduct\\nenforcement ladder\", 'https://github.com/mozilla/diversity')], 'code_blocks': [('text', '## üß© Browser Extension Usage\\n<!-- markdownlint-disable MD033 -->\\n<a href=\"https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood\" target=\"_blank\" title=\"Get Gitingest Extension from Chrome Web Store\"><img height=\"48\" src=\"https://github.com/user-attachments/assets/20a6e44b-fd46-4e6c-8ea6-aad436035753\" alt=\"Available in the Chrome Web Store\" /></a>\\n<a href=\"https://addons.mozilla.org/firefox/addon/gitingest\" target=\"_blank\" title=\"Get Gitingest Extension from Firefox Add-ons\"><img height=\"48\" src=\"https://github.com/user-attachments/assets/c0e99e6b-97cf-4af2-9737-099db7d3538b\" alt=\"Get The Add-on for Firefox\" /></a>\\n<a href=\"https://microsoftedge.microsoft.com/addons/detail/nfobhllgcekbmpifkjlopfdfdmljmipf\" target=\"_blank\" title=\"Get Gitingest Extension from Firefox Add-ons\"><img height=\"48\" src=\"https://github.com/user-attachments/assets/204157eb-4cae-4c0e-b2cb-db514419fd9e\" alt=\"Get from the Edge Add-ons\" /></a>\\n<!-- markdownlint-enable MD033 -->\\nThe extension is open source at [lcandy2/gitingest-extension](https://github.com/lcandy2/gitingest-extension).\\nIssues and feature requests are welcome to the repo.\\n## üí° Command line usage\\nThe `gitingest` command line tool allows you to analyze codebases and create a text dump of their contents.\\n'), ('text', 'This will write the digest in a text file (default `digest.txt`) in your current working directory.\\n## üêõ Python package usage\\n'), ('text', \"By default, this won't write a file but can be enabled with the `output` argument.\\n## üåê Self-host\\n1. Build the image:\\n   \"), ('text', '2. Run the container:\\n   '), ('text', 'The application will be available at `http://localhost:8000`.\\nIf you are hosting it on a domain, you can specify the allowed hostnames via env variable `ALLOWED_HOSTS`.\\n   '), ('text', '## ‚úîÔ∏è Contributing to Gitingest\\n### Non-technical ways to contribute\\n- **Create an Issue**: If you find a bug or have an idea for a new feature, please [create an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub. This will help us track and prioritize your request.\\n- **Spread the Word**: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.\\n- **Use Gitingest**: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by [creating an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub or by reaching out to us on [Discord](https://discord.com/invite/zerRaGK9EC).\\n### Technical ways to contribute\\nGitingest aims to be friendly for first time contributors, with a simple python and html codebase. If you need any help while working with the code, reach out to us on [Discord](https://discord.com/invite/zerRaGK9EC). For detailed instructions on how to make a pull request, see [CONTRIBUTING.md](./CONTRIBUTING.md).\\n## üõ†Ô∏è Stack\\n- [Tailwind CSS](https://tailwindcss.com) - Frontend\\n- [FastAPI](https://github.com/fastapi/fastapi) - Backend framework\\n- [Jinja2](https://jinja.palletsprojects.com) - HTML templating\\n- [tiktoken](https://github.com/openai/tiktoken) - Token estimation\\n- [apianalytics.dev](https://www.apianalytics.dev) - Simple Analytics\\n### Looking for a JavaScript/Node package?\\nCheck out the NPM alternative üì¶ Repomix: <https://github.com/yamadashy/repomix>\\n## Project Growth\\n[![Star History Chart](https://api.star-history.com/svg?repos=cyclotruc/gitingest&type=Date)](https://star-history.com/#cyclotruc/gitingest&Date)\\n\\n================================================\\nFile: /CONTRIBUTING.md\\n================================================\\n# Contributing to Gitingest\\nThanks for your interest in contributing to Gitingest! üöÄ Gitingest aims to be friendly for first time contributors, with a simple python and html codebase. We would love your help to make it even better. If you need any help while working with the code, please reach out to us on [Discord](https://discord.com/invite/zerRaGK9EC).\\n## How to Contribute (non-technical)\\n- **Create an Issue**: If you find a bug or have an idea for a new feature, please [create an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub. This will help us track and prioritize your request.\\n- **Spread the Word**: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.\\n- **Use Gitingest**: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by [creating an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub or by reaching out to us on [Discord](https://discord.com/invite/zerRaGK9EC).\\n## How to submit a Pull Request\\n1. Fork the repository.\\n2. Clone the forked repository:\\n   '), ('text', '3. Set up the development environment and install dependencies:\\n   '), ('text', '4. Create a new branch for your changes:\\n    '), ('text', '5. Make your changes. Make sure to add corresponding tests for your changes.\\n6. Stage your changes:\\n    '), ('text', '7. Run the tests:\\n   '), ('text', '8. Run the app locally using Docker to test your changes (optional):\\n   1. Build the Docker image\\n        '), ('text', '   2. Run the Docker container:\\n      '), ('text', '   3. Open your browser and navigate to `http://localhost:8000` to see the app running.\\n9. Confirm that everything is working as expected. If you encounter any issues, fix them and repeat steps 6 to 8.\\n10. Commit your changes:\\n    '), ('text', '    If `pre-commit` raises any issues, fix them and repeat steps 6 to 9.\\n11. Push your changes:\\n    ')], 'frontmatter': None}, 'repository': {'repository_url': 'https://github.com/cyclotruc/gitingest', 'repository_name': 'gitingest', 'organization': 'cyclotruc', 'total_files': 1, 'file_types': {}, 'directory_structure': {}}}\n",
      "Collection my_repo_collection already exists\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 6. Add documents to vector store\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:287\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    286\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:441\u001b[0m, in \u001b[0;36mQdrantVectorStore.add_texts\u001b[0;34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add texts with embeddings to the vectorstore.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m added_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 441\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43madded_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:951\u001b[0m, in \u001b[0;36mQdrantVectorStore._generate_batches\u001b[0;34m(self, texts, metadatas, ids, batch_size)\u001b[0m\n\u001b[1;32m    941\u001b[0m batch_metadatas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(islice(metadatas_iterator, batch_size)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    942\u001b[0m batch_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(islice(ids_iterator, batch_size))\n\u001b[1;32m    943\u001b[0m points \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    944\u001b[0m     models\u001b[38;5;241m.\u001b[39mPointStruct(\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mpoint_id,\n\u001b[1;32m    946\u001b[0m         vector\u001b[38;5;241m=\u001b[39mvector,\n\u001b[1;32m    947\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m point_id, vector, payload \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    950\u001b[0m         batch_ids,\n\u001b[0;32m--> 951\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_payloads(\n\u001b[1;32m    953\u001b[0m             batch_texts,\n\u001b[1;32m    954\u001b[0m             batch_metadatas,\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_payload_key,\n\u001b[1;32m    956\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_payload_key,\n\u001b[1;32m    957\u001b[0m         ),\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m ]\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m batch_ids, points\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:992\u001b[0m, in \u001b[0;36mQdrantVectorStore._build_vectors\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_vectors\u001b[39m(\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    989\u001b[0m     texts: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    990\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[models\u001b[38;5;241m.\u001b[39mVectorStruct]:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_mode \u001b[38;5;241m==\u001b[39m RetrievalMode\u001b[38;5;241m.\u001b[39mDENSE:\n\u001b[0;32m--> 992\u001b[0m         batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    994\u001b[0m             {\n\u001b[1;32m    995\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_name: vector,\n\u001b[1;32m    996\u001b[0m             }\n\u001b[1;32m    997\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m vector \u001b[38;5;129;01min\u001b[39;00m batch_embeddings\n\u001b[1;32m    998\u001b[0m         ]\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_mode \u001b[38;5;241m==\u001b[39m RetrievalMode\u001b[38;5;241m.\u001b[39mSPARSE:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/langchain_ollama/embeddings.py:161\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed search docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     embedded_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded_docs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/ollama/_client.py:356\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, model, input, truncate, options, keep_alive)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[1;32m    349\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EmbedResponse:\n\u001b[0;32m--> 356\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/embed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/ollama/_client.py:177\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/ollama/_client.py:118\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 118\u001b[0m   r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "repo_url = \"https://github.com/cyclotruc/gitingest\"\n",
    "summary, tree, content = ingest(repo_url)\n",
    "print(summary)\n",
    "# 4. Process and index the repository\n",
    "chunks = advanced_repository_chunking(content, repo_url)\n",
    "vector_size = len(embeddings.embed_query(\"test\"))\n",
    "# Create the collection first\n",
    "collection_name = \"my_repo_collection\"\n",
    "\n",
    "\n",
    "# Check if collection exists and create if it doesn't\n",
    "if not qdrant.collection_exists(collection_name):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=rest.VectorParams(\n",
    "            size=vector_size,\n",
    "            distance=rest.Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    print(f\"Created new collection: {collection_name}\")\n",
    "else:\n",
    "    print(f\"Collection {collection_name} already exists\")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Add documents to vector store\n",
    "vector_store.add_documents(chunks)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebe60b-b378-4e38-bbf7-0f032ea69538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search\n",
    "results = vector_store.similarity_search(\n",
    "    \"What is the purpose of this repository?\",\n",
    "    k=2\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f47a3-2938-4ade-93bd-4d7655f29ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from typing import List, Dict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "class LocalRAG:\n",
    "    \"\"\"A class to handle local RAG operations using Ollama for both embeddings and LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, model_name=\"phi3.5\"):\n",
    "        \"\"\"\n",
    "        Initialize the LocalRAG with vector store and model configurations.\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Initialized QdrantVectorStore\n",
    "            model_name: Name of the Ollama model to use (default: \"phi3.5\")\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = ChatOllama(model=model_name, base_url=\"http://ollama:11434\")\n",
    "        \n",
    "        # Define a better prompt template for RAG\n",
    "        self.template = \"\"\"You are a helpful AI assistant. Use the following context to answer the question. \n",
    "        If you cannot find the answer in the context, say \"I cannot find the answer in the provided context.\"\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question:\n",
    "        {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            template=self.template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "    def format_docs(self, docs: List[Dict]) -> str:\n",
    "        \"\"\"Format the retrieved documents into a string.\"\"\"\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def retrieve_and_answer(self, question: str, k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents and generate an answer.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            k: Number of documents to retrieve (default: 3)\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated answer\n",
    "        \"\"\"\n",
    "        # First retrieve the documents\n",
    "        retrieved_docs = self.vector_store.similarity_search(question, k=k)\n",
    "        formatted_context = self.format_docs(retrieved_docs)\n",
    "        \n",
    "        # Create and execute the RAG chain\n",
    "        chain = (\n",
    "            self.prompt | \n",
    "            self.llm | \n",
    "            StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Execute the chain with the prepared context and question\n",
    "        response = chain.invoke({\n",
    "            \"context\": formatted_context,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_relevant_chunks(self, question: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get the relevant chunks for a question without generating an answer.\n",
    "        Useful for debugging and understanding what context is being used.\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            k: Number of documents to retrieve (default: 3)\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of relevant documents\n",
    "        \"\"\"\n",
    "        return self.vector_store.similarity_search(question, k=k)\n",
    "\n",
    "def demonstrate_local_rag(vector_store):\n",
    "    \"\"\"Demonstrate how to use the LocalRAG class.\"\"\"\n",
    "    # Initialize the RAG system\n",
    "    rag = LocalRAG(vector_store)\n",
    "    \n",
    "    # Example questions to test\n",
    "    questions = [\n",
    "        # \"What is the purpose of this repository?\",\n",
    "        # \"How does the code handle different file types?\",\n",
    "        # \"What metadata is extracted from Python files?\"\n",
    "        \"How can I use this library?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"RAG Demo:\\n\")\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"\\nRelevant chunks:\")\n",
    "        chunks = rag.get_relevant_chunks(question, k=2)\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"\\nChunk {i}:\")\n",
    "            print(f\"Source: {chunk.metadata.get('file_path', 'Unknown')}\")\n",
    "            print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "        \n",
    "        print(\"\\nGenerated Answer:\")\n",
    "        answer = rag.retrieve_and_answer(question)\n",
    "        print(answer)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# To use this code with your existing setup:\n",
    "\"\"\"\n",
    "# First make sure you have the vector store set up as in your original code\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant,\n",
    "    collection_name=\"my_repo_collection\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "vector_store\n",
    "# Then you can run the demonstration\n",
    "demonstrate_local_rag(vector_store)\n",
    "\"\"\"\n",
    "demonstrate_local_rag(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a5af2-e6ce-47eb-8ad6-f1f98c5e9188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
