{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c64f80-e612-48b0-b4b1-bf3e0be2595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "ERROR:root:This is a log message\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# allow loading modules from local directory.\n",
    "sys.path.insert(1, '/home/jovyan/work/code')\n",
    "\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.resources import SERVICE_NAME, Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.instrumentation.langchain import LangchainInstrumentor\n",
    "\n",
    "# Logging (Experimental)\n",
    "from opentelemetry._logs import set_logger_provider\n",
    "from opentelemetry.exporter.otlp.proto.grpc._log_exporter import (\n",
    "    OTLPLogExporter,\n",
    ")\n",
    "from opentelemetry.sdk._logs import LoggerProvider, LoggingHandler\n",
    "from opentelemetry.sdk._logs.export import BatchLogRecordProcessor\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "\n",
    "#from otel_grpc import configure_otel_otlp\n",
    "\n",
    "from config import VectorDBConfig, EmbeddingConfig, ProcessingConfig, ChatConfig\n",
    "from pipeline import DocumentPipeline\n",
    "from localrag import LocalRAG\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio \n",
    "import logging\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialise and Setup OpenTelemetry for the session\n",
    "resource = Resource(attributes={\n",
    "  SERVICE_NAME:  os.getenv('OTEL_SERVICE_NAME', 'jupyter-demo')\n",
    "})\n",
    "provider = TracerProvider(resource=resource)\n",
    "processor = BatchSpanProcessor(OTLPSpanExporter(endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\")))\n",
    "provider.add_span_processor(processor)\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    " # Configure Logging\n",
    "#configure_otel_otlp( os.getenv('OTEL_SERVICE_NAME', 'jupyter-demo'), endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\"))\n",
    "\n",
    "logger_provider = LoggerProvider(\n",
    "    resource=resource\n",
    ")\n",
    "set_logger_provider(logger_provider)\n",
    "\n",
    "exporter = OTLPLogExporter(insecure=True)\n",
    "logger_provider.add_log_record_processor(BatchLogRecordProcessor(exporter))\n",
    "handler = LoggingHandler(level=logging.NOTSET, logger_provider=logger_provider)\n",
    "\n",
    "# Attach OTLP handler to root logger\n",
    "logging.getLogger().addHandler(handler)\n",
    "\n",
    "# logging.basicConfig()\n",
    "#logging.basicConfig(format = \"%(asctime)s:%(levelname)s:%(message)s\", level = logging.DEBUG)\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "LangchainInstrumentor().instrument()\n",
    "logger = logging.getLogger(__name__)\n",
    "tracer = trace.get_tracer(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64272a1-7eed-49e5-8b17-67880bf35215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the connection strings and configuration passed from Aspire AppHost\n",
    "\n",
    "def parse_ollama_connection(conn_str):\n",
    "    parts = conn_str.split(';')\n",
    "    endpoint = next(p.split('=')[1] for p in parts if p.startswith('Endpoint='))\n",
    "    model = next(p.split('=')[1] for p in parts if p.startswith('Model='))\n",
    "    return endpoint, model\n",
    "\n",
    "chat_conn = os.getenv('ConnectionStrings__chat-model', 'Endpoint=http://ollama:11434;Model=phi3.5')\n",
    "chat_model_url, chat_model_id = parse_ollama_connection(chat_conn)\n",
    "\n",
    "embeddings_conn = os.getenv('ConnectionStrings__embedding-model', 'Endpoint=http://ollama:11434;Model=mxbai-embed-large')\n",
    "embedding_model_url, embeddings_model = parse_ollama_connection(embeddings_conn)\n",
    "\n",
    "qdrant_conn = os.getenv('ConnectionStrings__qdrant_http', 'Endpoint=http://qdrant:6334;Key=aMjJKx0t1a6E9hysaCacWz')\n",
    "parts = qdrant_conn.split(';')\n",
    "qdrant_url = next(p.split('=')[1] for p in parts if p.startswith('Endpoint='))\n",
    "qdrant_key = next(p.split('=')[1] for p in parts if p.startswith('Key='))\n",
    "\n",
    "vector_db_config = VectorDBConfig(\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_key,\n",
    "    collection_name=\"embedding-demo-aspire-inc-yml\"\n",
    ")\n",
    "chat_config = ChatConfig(\n",
    "    model_name=chat_model_id,\n",
    "    base_url=chat_model_url\n",
    ")\n",
    "embedding_config = EmbeddingConfig(\n",
    "    model_name=embeddings_model,\n",
    "    base_url=embedding_model_url\n",
    ")\n",
    "processing_config = ProcessingConfig(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_metadata=True,\n",
    "    extract_code_entities=True\n",
    ")\n",
    "\n",
    "print(vector_db_config)\n",
    "print(embedding_config)\n",
    "print(chat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757e336-7b98-44af-ae16-716b15d446bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest a GitHub repository and import into our vector store\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = DocumentPipeline(\n",
    "    vector_db_config=vector_db_config,\n",
    "    embedding_config=embedding_config, \n",
    "    processing_config=processing_config\n",
    ")\n",
    "\n",
    "###Process git repository\n",
    "#pipeline.process_repository(\"https://github.com/dotnet/docs-aspire\")\n",
    "\n",
    "# Or process local directory\n",
    "#pipeline.process_local_directory(\"./docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855ef1f-7eca-4c4b-897d-e00d786b8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our RAG Solution\n",
    "\n",
    "def demonstrate_local_rag(rag):\n",
    "    \"\"\"Demonstrate how to use the LocalRAG class.\"\"\"    \n",
    "    # Example questions to test\n",
    "    questions = [\n",
    "        \"Why should I know about .Net Aspire?\",\n",
    "        \"Is .Net Aspire an alternative to Kubernetes?\"\n",
    "    ]\n",
    "    with tracer.start_as_current_span(\"Entering questions loop.\"):\n",
    "        for question in questions:\n",
    "            print(f\"Question: {question}\")\n",
    "            print(\"\\nRelevant chunks:\")\n",
    "            #with tracer.start_as_current_span(\"rag get chunks\"):\n",
    "            # chunks = rag.get_relevant_chunks(question, k=5)\n",
    "            # for i, chunk in enumerate(chunks, 1):\n",
    "            #     print(f\"\\nChunk {i}:\")\n",
    "            #     print(f\"Source: {chunk.metadata.get('file_path', 'Unknown')}\")\n",
    "            #      print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "                \n",
    "            print(\"\\nGenerated Answer:\")\n",
    "            with tracer.start_as_current_span(\"Retrieve answers.\"):\n",
    "                answer = rag.retrieve_and_answer(question, k=6)\n",
    "                print(answer)\n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "rag = LocalRAG(\n",
    "    vector_db_config=vector_db_config,\n",
    "    embedding_config=embedding_config, \n",
    "    chat_config=chat_config\n",
    ")\n",
    "with tracer.start_as_current_span(\"Starting demo\"):\n",
    "    demonstrate_local_rag(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03d601-a3d6-4de9-80c9-6109d86594a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
